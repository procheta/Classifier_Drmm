#!/bin/sh
#SBATCH -N 1    # how many nodes are required (in most cases it is 1)
#SBATCH -t 1-12:30:00    # maximum execution time, in this case one day, two hours and thirty minutes (optional)
#SBATCH --mem=30000
#SBATCH -p compute    # which partition to run on ('compute' is default)
#SBATCH -J wordVecTrain    # arbitrary name for the job (you choose)

# load modules if needed (modules are used in the Boole cluster only)
#module load staskfarm
# check which modules are available
#module av

# uncomment the following to get a log of memory usage
# vmstat -S M {interval_secs} >> memory_usage_$SLURM_JOBID.log &

#for I in 0.1 0.2 0.5 0.6 0.7 0.8 1.0
#/home/procheta/pythonNew/bin/python3
#do
#python /home/procheta/writeGoogleNewsVector.py /home/procheta/word2vec/GoogleNews-vectors-negative300.bin  /home/procheta/vecIput.txt  
#/home/procheta/spinning-storage/procheta/pythonNew/bin/python3 L2xReproduce.py

#/home/procheta/anaconda3/envs/DRMM/bin/python DRMM.py
#/home/procheta/spinning-storage/procheta/pythonNew/bin/python3 BiasL2x_NegScore.py
#for i in 0.1 0.2 0.3 0.4 0.5
#do
#/home/procheta/spinning-storage/procheta/pythonNew/bin/python3 network.py processed_orcas.csv

python savebertvecs.py /home/procheta/left_msmarco_text.txt /home/procheta/left_msmarco.vec

#java DRMMPreprocessing 
#cd /home/procheta/spinning-storage/procheta/jrandwalk/jrandwalk/target/classes

#for i in 0.5  
#do
#java graphwalk.RWalk  "/home/procheta/spinning-storage/procheta/edge_file_10000_1_"$i".txt" 0.7 0.2 50 5 26  false "Biased_Random_Walk"
#python node2vec_new.py
#done
#echo "Complete one loop"
#done
#echo hello

